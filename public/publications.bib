@misc{xie2025maverix,
    author = {Liuyue Xie and George Z. Wei and Avik Kuthiala and Ce Zheng and Ananya Bal and Mosam Dabhi and Liting Wen and Taru Rustagi and Ethan Lai and Sushil Khyalia and Rohan Choudhury and Morteza Ziyadi and Xu Zhang and Hao Yang and László A. Jeni},
    author+an = {1=first; 2=first; 3=first;},
    title = {MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX},
    year = {2025},
    abstract = {Frontier models have either been language-only or have primarily focused on vision and language modalities. Although recent advancements in models with vision and audio understanding capabilities have shown substantial progress, the field lacks a standardized evaluation framework for thoroughly assessing their cross-modality perception performance. We introduce MAVERIX~(Multimodal Audio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and 2,556 questions explicitly designed to evaluate multimodal models through tasks that necessitate close integration of video and audio information. MAVERIX uniquely provides models with audiovisual tasks, closely mimicking the multimodal perceptual experiences available to humans during inference and decision-making processes. To our knowledge, MAVERIX is the first benchmark aimed explicitly at assessing comprehensive audiovisual integration. Experiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show performance approaching human levels (around 70% accuracy), while human experts reach near-ceiling performance (95.1%). With standardized evaluation protocols, a rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a challenging testbed for advancing audiovisual multimodal intelligence.},
    url = {https://arxiv.org/abs/2503.21699},
    month = {3},
    eprint = {2503.21699},
    archiveprefix = {arXiv},
    teaser = {images/maverix.jpg}
}

@inproceedings{dooley2022robustness,
    author = {Dooley, Samuel and Wei, George Z. and Goldstein, Tom and Dickerson, John},
    booktitle = {Advances in Neural Information Processing Systems},
    title = {Robustness Disparities in Face Detection},
    year = {2022},
    abstract = {Facial analysis systems have been deployed by large companies and critiqued by scholars and activists for the past decade. Many existing algorithmic audits examine the performance of these systems on later stage elements of facial analysis systems like facial recognition and age, emotion, or perceived gender prediction; however, a core component to these systems has been vastly understudied from a fairness perspective: face detection, sometimes called face localization. Since face detection is a pre-requisite step in facial analysis systems, the bias we observe in face detection will flow downstream to the other components like facial recognition and emotion prediction. Additionally, no prior work has focused on the robustness of these systems under various perturbations and corruptions, which leaves open the question of how various people are impacted by these phenomena. We present the first of its kind detailed benchmark of face detection systems, specifically examining the robustness to noise of commercial and academic models. We use both standard and recently released academic facial datasets to quantitatively analyze trends in face detection robustness. Across all the datasets and systems, we generally find that photos of individuals who are \emph{masculine presenting}, \emph{older}, of \emph{darker skin type}, or have \emph{dim lighting} are more susceptible to errors than their counterparts in other identities.},
    url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/f9faef4e1b4dbbd48ef60056ffe14c90-Abstract-Datasets_and_Benchmarks.html},
    month = {11},
    editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
    pages = {38245--38259},
    publisher = {Curran Associates, Inc.},
    volume = {35},
    teaser = {images/robustness.png},
    project = {https://spamueldooley.com/robustness/},
    github = {https://github.com/dooleys/robustness}
}

@inproceedings{hattimare2022maruna,
    author = {Hattimare, Amit and Dharawat, Arkin and Khan, Yelman and Lien, Yen-Chieh and Samarinas, Chris and Wei, George Z. and Yang, Yulin and Zamani, Hamed},
    booktitle = {Alexa Prize TaskBot Challenge Proceedings},
    title = {Maruna Bot: An Extensible Retrieval-Focused Framework for Task-Oriented Dialogues},
    year = {2022},
    abstract = {We present Maruna Bot, a Task-Oriented Dialogue System (TODS) that assists people in cooking or Do-It-Yourself (DIY) tasks using either a speech-only or multimodal (speech and screen) interface. Building such a system is challenging, because it touches many research areas including language understanding, text generation, task planning, dialogue state tracking, question answering, multi-modal retrieval, instruction summarization, robustness, and result presentation, among others. Our bot lets users choose their desired tasks with flexible phrases, uses multi-stage intent classification, asks clarifying questions to improve retrieval, supports in-task and open-domain Question Answering throughout the conversation, effectively maintains the task status, performs query expansion and instruction re-ranking using both textual and visual signals.},
    url = {https://www.amazon.science/alexa-prize/proceedings/maruna-bot-an-extensible-retrieval-focused-framework-for-task-oriented-dialogues},
    month = {6},
    teaser = {images/maruna.png}
}

@misc{dooley2022commercial,
    author = {Samuel Dooley and George Z. Wei and Tom Goldstein and John P. Dickerson},
    author+an = {1=first; 2=first;},
    title = {Are Commercial Face Detection Models as Biased as Academic Models?},
    year = {2022},
    abstract = {As facial recognition systems are deployed more widely, scholars and activists have studied their biases and harms. Audits are commonly used to accomplish this and compare the algorithmic facial recognition systems' performance against datasets with various metadata labels about the subjects of the images. Seminal works have found discrepancies in performance by gender expression, age, perceived race, skin type, etc. These studies and audits often examine algorithms which fall into two categories: academic models or commercial models. We present a detailed comparison between academic and commercial face detection systems, specifically examining robustness to noise. We find that state-of-the-art academic face detection models exhibit demographic disparities in their noise robustness, specifically by having statistically significant decreased performance on older individuals and those who present their gender in a masculine manner. When we compare the size of these disparities to that of commercial models, we conclude that commercial models - in contrast to their relatively larger development budget and industry-level fairness commitments - are always as biased or more biased than an academic model.},
    url = {https://arxiv.org/abs/2201.10047v1},
    month = {1},
    eprint = {2201.10047},
    archiveprefix = {arXiv},
    teaser = {images/commercial.jpg}
}

@misc{wei2021analysis,
    author = {Wei, George Z. and Fiterau, Ina and Iyyer, Mohit},
    title = {An Analysis of Low Recall Text-Image Retrieval with LXMERT and CLIP},
    year = {2021},
    abstract = {Previously, deep neural networks have found widespread success in computer vision due primarily to convolutional neural networks and the expressive feature extraction capabilities they have. More recently, deep learning has had a great impact on natural language processing with the introduction of attention and as a direct result Transformers and finally BERT. One such task that belongs in the intersection of these two subfields is the task of text-image retrieval, which is composed of text-based image retrieval (given a query text, retrieve the most relevant images) and image-based text retrieval (given a query image, retrieve the most relevant text/caption). In the past, models would be trained from scratch on this task. The recent paradigm shift to pretrain-finetune in deep learning has given rise to vision-language pretrained models which learn image and text associations during pretraining and get fine-tuned for vision-language downstream tasks. LXMERT, one such model, relies on a Faster R-CNN, a pretrained object detector model, to generate object detection features from the images and learns the joint associations of the modalities through some cross-modal Transformer blocks. In contrast, CLIP utilizes two separate but similar encoders for the two modalities — a Vision Transformer (ViT) for the images and a regular Transformer for the text. In this paper, we analyze the performance of these two VLP models on the MSCOCO dataset by looking into the the patterns low recall queries and when CLIP outperforms LXMERT.},
    url = {https://gzhihongwei.github.io/papers/wei2021analysis.pdf},
    month = {12},
    note = {Undergraduate Honors Thesis},
    teaser = {images/analysis.jpg}
}

@misc{dooley2021comparing,
    author = {Samuel Dooley and Ryan Downing and George Z. Wei and Nathan Shankar and Bradon Thymes and Gudrun Thorkelsdottir and Tiye Kurtz-Miott and Rachel Mattson and Olufemi Obiwumi and Valeriia Cherepanova and Micah Goldblum and John P. Dickerson and Tom Goldstein},
    author+an = {1=first; 2=first; 3=first;},
    title = {Comparing Human and Machine Bias in Face Recognition},
    year = {2021},
    abstract = {Much recent research has uncovered and discussed serious concerns of bias in facial analysis technologies, finding performance disparities between groups of people based on perceived gender, skin type, lighting condition, etc. These audits are immensely important and successful at measuring algorithmic bias but have two major challenges: the audits (1) use facial recognition datasets which lack quality metadata, like LFW and CelebA, and (2) do not compare their observed algorithmic bias to the biases of their human alternatives. In this paper, we release improvements to the LFW and CelebA datasets which will enable future researchers to obtain measurements of algorithmic bias that are not tainted by major flaws in the dataset (e.g. identical images appearing in both the gallery and test set). We also use these new data to develop a series of challenging facial identification and verification questions that we administered to various algorithms and a large, balanced sample of human reviewers. We find that both computer models and human survey participants perform significantly better at the verification task, generally obtain lower accuracy rates on dark-skinned or female subjects for both tasks, and obtain higher accuracy rates when their demographics match that of the question. Computer models are observed to achieve a higher level of accuracy than the survey participants on both tasks and exhibit bias to similar degrees as the human survey participants.},
    url = {https://arxiv.org/abs/2110.08396},
    month = {10},
    eprint = {2110.08396},
    archiveprefix = {arXiv},
    primaryclass = {cs.CV},
    teaser = {images/comparing.jpg}
}

