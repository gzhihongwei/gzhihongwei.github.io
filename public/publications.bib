@article{dooley2021comparinghumanmachinebias,
  title         = {Comparing Human and Machine Bias in Face Recognition},
  author        = {Samuel Dooley and Ryan Downing and George Wei and Nathan Shankar and Bradon Thymes and Gudrun Thorkelsdottir and Tiye Kurtz-Miott and Rachel Mattson and Olufemi Obiwumi and Valeriia Cherepanova and Micah Goldblum and John P Dickerson and Tom Goldstein},
  author+an     = {1=first; 2=first; 3=first;},
  year          = {2021},
  month         = {10},
  eprint        = {2110.08396},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  journal       = {arXiv},
  url           = {https://arxiv.org/abs/2110.08396},
  abstract      = {Much recent research has uncovered and discussed serious concerns of bias in facial analysis technologies, finding performance disparities between groups of people based on perceived gender, skin type, lighting condition, etc. These audits are immensely important and successful at measuring algorithmic bias but have two major challenges: the audits (1) use facial recognition datasets which lack quality metadata, like LFW and CelebA, and (2) do not compare their observed algorithmic bias to the biases of their human alternatives. In this paper, we release improvements to the LFW and CelebA datasets which will enable future researchers to obtain measurements of algorithmic bias that are not tainted by major flaws in the dataset (e.g. identical images appearing in both the gallery and test set). We also use these new data to develop a series of challenging facial identification and verification questions that we administered to various algorithms and a large, balanced sample of human reviewers. We find that both computer models and human survey participants perform significantly better at the verification task, generally obtain lower accuracy rates on dark-skinned or female subjects for both tasks, and obtain higher accuracy rates when their demographics match that of the question. Computer models are observed to achieve a higher level of accuracy than the survey participants on both tasks and exhibit bias to similar degrees as the human survey participants.}
}

@misc{wei2021analysis,
  title    = {An Analysis of Low Recall Text-Image Retrieval with LXMERT and CLIP},
  author   = {Wei, George Z. and Fiterau, Ina and Iyyer, Mohit},
  year     = 2021,
  month    = {12},
  journal  = {Undergraduate Honors Thesis},
  url      = {https://gzhihongwei.github.io/papers/wei2021analysis.pdf},
  abstract = {Previously, deep neural networks have found widespread success in computer vision due primarily to convolutional neural networks and the expressive feature extraction capabilities they have. More recently, deep learning has had a great impact on natural language processing with the introduction of attention and as a direct result Transformers and finally BERT. One such task that belongs in the intersection of these two subfields is the task of text-image retrieval, which is composed of text-based image retrieval (given a query text, retrieve the most relevant images) and image-based text retrieval (given a query image, retrieve the most relevant text/caption). In the past, models would be trained from scratch on this task. The recent paradigm shift to pretrain-finetune in deep learning has given rise to vision-language pretrained models which learn image and text associations during pretraining and get fine-tuned for vision-language downstream tasks. LXMERT, one such model, relies on a Faster R-CNN, a pretrained object detector model, to generate object detection features from the images and learns the joint associations of the modalities through some cross-modal Transformer blocks. In contrast, CLIP utilizes two separate but similar encoders for the two modalities — a Vision Transformer (ViT) for the images and a regular Transformer for the text. In this paper, we analyze the performance of these two VLP models on the MSCOCO dataset by looking into the the patterns low recall queries and when CLIP outperforms LXMERT.}
}

@article{dooley2022commercial,
  title     = {Are Commercial Face Detection Models as Biased as Academic Models?},
  author    = {Dooley, Samuel and Wei, George Z. and Goldstein, Tom, and Dickerson, John P.},
  author+an = {1=first; 2=first;},
  year      = 2022,
  month     = {1},
  journal   = {arXiv},
  url       = {https://arxiv.org/abs/2201.10047v1},
  abstract  = {As facial recognition systems are deployed more widely, scholars and activists have studied their biases and harms. Audits are commonly used to accomplish this and compare the algorithmic facial recognition systems' performance against datasets with various metadata labels about the subjects of the images. Seminal works have found discrepancies in performance by gender expression, age, perceived race, skin type, etc. These studies and audits often examine algorithms which fall into two categories: academic models or commercial models. We present a detailed comparison between academic and commercial face detection systems, specifically examining robustness to noise. We find that state-of-the-art academic face detection models exhibit demographic disparities in their noise robustness, specifically by having statistically significant decreased performance on older individuals and those who present their gender in a masculine manner. When we compare the size of these disparities to that of commercial models, we conclude that commercial models — in contrast to their relatively larger development budget and industry-level fairness commitments — are always as biased or more biased than an academic model.}
}

@inproceedings{hattimare2022maruna,
  title     = {Maruna Bot: An Extensible Retrieval-Focused Framework for Task-Oriented Dialogues},
  author    = {Hattimare, Amit and Dharawat, Arkin and Khan, Yelman and Lien, Yen-Chieh and Samarinas, Chris and Wei, George Z. and Yang, Yulin and Zamani, Hamed},
  year      = 2022,
  month     = {6},
  booktitle = {Alexa Prize TaskBot Challenge Proceedings},
  url       = {https://www.amazon.science/alexa-prize/proceedings/maruna-bot-an-extensible-retrieval-focused-framework-for-task-oriented-dialogues},
  abstract  = {We present Maruna Bot, a Task-Oriented Dialogue System (TODS) that assists people in cooking or Do-It-Yourself (DIY) tasks using either a speech-only or multimodal (speech and screen) interface. Building such a system is challenging, because it touches many research areas including language understanding, text generation, task planning, dialogue state tracking, question answering, multi-modal retrieval, instruction summarization, robustness, and result presentation, among others. Our bot lets users choose their desired tasks with flexible phrases, uses multi-stage intent classification, asks clarifying questions to improve retrieval, supports in-task and open-domain Question Answering throughout the conversation, effectively maintains the task status, performs query expansion and instruction re-ranking using both textual and visual signals.}
}

@inproceedings{dooley2022robustness,
  author    = {Dooley, Samuel and Wei, George Z. and Goldstein, Tom and Dickerson, John},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {38245--38259},
  publisher = {Curran Associates, Inc.},
  title     = {Robustness Disparities in Face Detection},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/f9faef4e1b4dbbd48ef60056ffe14c90-Abstract-Datasets_and_Benchmarks.html},
  volume    = {35},
  year      = {2022},
  month     = {11},
  abstract  = {Facial analysis systems have been deployed by large companies and critiqued by scholars and activists for the past decade. Many existing algorithmic audits examine the performance of these systems on later stage elements of facial analysis systems like facial recognition and age, emotion, or perceived gender prediction; however, a core component to these systems has been vastly understudied from a fairness perspective: face detection, sometimes called face localization. Since face detection is a pre-requisite step in facial analysis systems, the bias we observe in face detection will flow downstream to the other components like facial recognition and emotion prediction. Additionally, no prior work has focused on the robustness of these systems under various perturbations and corruptions, which leaves open the question of how various people are impacted by these phenomena. We present the first of its kind detailed benchmark of face detection systems, specifically examining the robustness to noise of commercial and academic models. We use both standard and recently released academic facial datasets to quantitatively analyze trends in face detection robustness. Across all the datasets and systems, we generally find that photos of individuals who are \emph{masculine presenting}, \emph{older}, of \emph{darker skin type}, or have \emph{dim lighting} are more susceptible to errors than their counterparts in other identities.}
}
